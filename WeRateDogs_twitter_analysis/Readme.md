# Wrangling Data - Archived Tweets from WeRateDogs

For this project I was tasked with wrangling archived tweet data from the WeRateDogs Twitter account. To this end we were provided with the archived tweet data as a CSV file and given the additional assignment of gathering extra data to supplement the tweet data. This report briefly describes what was done as well as the challenges encountered at each step of the data wrangling process.

## Gather
As part of the project requirements, two pieces of supplementary data were gathered from different sources. The first piece of data was the prediction results from a machine learning algorithm trained on the images from a sample of the archived tweet data. This data was the most straightforward of the two pieces of data to retrive since it was a simple matter of fetching the file containing the image prediction results from the provided link and saving it to the current working directory.

The second piece of data was, at minimum, the retweet and favorite counts of each tweet contained in the archive. Obtaining this data was was a bit more involved than retrieving the image prediction results. Rather than being given a link to a file containing the data we had to gather the data from the WeRateDogs Twitter account. Here is where we encountered the first challenge of our data wrangling endeavor: understanding the structure of the tweet object JSON. While this challenge was overcome fairly easily, much time was spent poring over the twitter object data dictionary in the official Twitter documentation.

## Assess
The assessment of the gathered data was done in two stages: visual assessment and programmatic assessment. During the visual assessment stage, we identified issues ranged from HTML tags surrounding the name of the platform from which the tweet was made to the individual categories of a variable being spread accross four columns. Programmatic assessment allowed us to discover more subtle quality and tidiness issues such as duplicate data, the presence of useless data, and date values in the ratings columns.

The twitter data used in this project was especially problematic as the data from some of the columns in the archived twitter data was taken from the text of each tweet. With unstructured text such as that contained in the text column it's difficult to extract the intended data. This difficulty is compounded by the inability to determine the context of the text. Thus, there are likely still errors in the data that haven't been documented.

## Clean
Cleaning the data followed the define-code-test process outlined in the lessons. For most of the quality and tidiness issues the process of cleaning was straightforward. However, there were a few tidiness and quality issues that were harder to resolve. The most difficult tidiness issue to fix was stacking the p, p_conf, and p*_dog columns in the image prediction results data. The built-in methods for reshaping data could not stack the columns in the desired manner and so we had to resort to writing our own code to do this.

The biggest challenge we encountered when cleaning the data was handling quality issues related to extracting the proper data from the text of each tweet as there were many cases that had to be taken into consideration. Some good examples of this would be the presence of dates and multiple scores which are difficult to fix with the standard text manipulation methods. Since it's difficult, if not impossible, to account for all possible cases there is likely some inaccurate data in the final cleaned dataset.
